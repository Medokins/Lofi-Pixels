{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow  as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, LeakyReLU, Dropout, Flatten, Activation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data\", \"selected\")\n",
    "batch_size = 64 # to test\n",
    "im_height, im_width = 128, 128\n",
    "\n",
    "data = tf.keras.preprocessing.image_dataset_from_directory(data_dir, label_mode=None, image_size=(im_height, im_width), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "normalized_data = data.map(lambda x: normalization_layer(x))\n",
    "\n",
    "# Test the normalization\n",
    "images = next(iter(normalized_data))\n",
    "print(images.numpy().min(), images.numpy().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = next(iter(normalized_data))\n",
    "\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "# for i in range(2):\n",
    "#     for j in range(2):\n",
    "#         axs[i, j].imshow(images[i*2+j])\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "g_resolution = 2\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(Dense(4*4*256, activation='relu', input_dim=latent_dim))\n",
    "generator.add(Reshape((4, 4, 256)))\n",
    "generator.add(UpSampling2D(size=(g_resolution, g_resolution)))\n",
    "generator.add(Conv2D(256, kernel_size=3, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(UpSampling2D(size=(g_resolution, g_resolution)))\n",
    "generator.add(Conv2D(256, kernel_size=3, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(UpSampling2D(size=(g_resolution, g_resolution))) # so the images are 256x256\n",
    "generator.add(Conv2D(256, kernel_size=3, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(UpSampling2D(size=(g_resolution, g_resolution)))\n",
    "generator.add(Conv2D(128, kernel_size=3, padding='same'))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(UpSampling2D(size=(g_resolution, g_resolution)))\n",
    "generator.add(Conv2D(3, kernel_size=3, padding='same'))\n",
    "generator.add(Activation('tanh'))\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([1, latent_dim])\n",
    "generated_image = generator(seed, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0])\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminator #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define discriminator\n",
    "d_resolution = 128\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(d_resolution, d_resolution, 3), padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.25))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discriminator_Verdict = discriminator(generated_image)\n",
    "print(Discriminator_Verdict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GAN class #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.discriminator_loss = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.generator_loss = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.discriminator_loss, self.generator_loss]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        generated_images = self.generator(seed)\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "        labels += 0.02137 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            discriminator_loss = self.loss_fn(labels, predictions)\n",
    "\n",
    "        grads = tape.gradient(discriminator_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(seed))\n",
    "            generator_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(generator_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        self.discriminator_loss.update_state(discriminator_loss)\n",
    "        self.generator_loss.update_state(generator_loss)\n",
    "        return {\"discriminator_loss\": self.discriminator_loss.result(), \"generator_loss\": self.generator_loss.result()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "discriminator_opt = tf.keras.optimizers.Adamax()\n",
    "generator_opt = tf.keras.optimizers.Adamax()\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "model = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "model.compile(d_optimizer=discriminator_opt, g_optimizer=generator_opt, loss_fn=loss_fn)\n",
    "\n",
    "history = model.fit(normalized_data, epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise\n",
    "latent_dim = 100\n",
    "num_images = 5\n",
    "noise = tf.random.normal([num_images, latent_dim])\n",
    "\n",
    "# Generate images using the generator model\n",
    "generator = model.generator\n",
    "generated_images = generator(noise, training=False)\n",
    "\n",
    "# Plot the generated images\n",
    "fig, axs = plt.subplots(nrows=1, ncols=num_images, figsize=(20, 20))\n",
    "for i in range(num_images):\n",
    "    axs[i].imshow(generated_images[i]) # Denormalize the image\n",
    "    axs[i].axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
